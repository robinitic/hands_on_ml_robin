{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  float64\n",
      " 3   total_rooms         20640 non-null  float64\n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  float64\n",
      " 6   households          20640 non-null  float64\n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16512 entries, 17606 to 15775\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           16512 non-null  float64\n",
      " 1   latitude            16512 non-null  float64\n",
      " 2   housing_median_age  16512 non-null  float64\n",
      " 3   total_rooms         16512 non-null  float64\n",
      " 4   total_bedrooms      16354 non-null  float64\n",
      " 5   population          16512 non-null  float64\n",
      " 6   households          16512 non-null  float64\n",
      " 7   median_income       16512 non-null  float64\n",
      " 8   ocean_proximity     16512 non-null  object \n",
      "dtypes: float64(8), object(1)\n",
      "memory usage: 1.3+ MB\n",
      "[[-1.15604281e+00  7.71949616e-01  7.43330892e-01 -4.93233934e-01\n",
      "  -4.45438207e-01 -6.36211407e-01 -4.20698422e-01 -6.14937444e-01\n",
      "  -3.12054519e-01 -8.64987054e-02  1.55317530e-01  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.17602483e+00  6.59694795e-01 -1.16531720e+00 -9.08966554e-01\n",
      "  -1.03692780e+00 -9.98331347e-01 -1.02222705e+00  1.33645936e+00\n",
      "   2.17683377e-01 -3.35339129e-02 -8.36289016e-01  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.18684903e+00 -1.34218285e+00  1.86641864e-01 -3.13659889e-01\n",
      "  -1.53344583e-01 -4.33639362e-01 -9.33177983e-02 -5.32045602e-01\n",
      "  -4.65315160e-01 -9.24049941e-02  4.22200402e-01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      " [-1.70676728e-02  3.13575763e-01 -2.90520160e-01 -3.62762167e-01\n",
      "  -3.96755937e-01  3.60409561e-02 -3.83435587e-01 -1.04556555e+00\n",
      "  -7.96612428e-02  8.97356110e-02 -1.96453142e-01  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 4.92473835e-01 -6.59299356e-01 -9.26736191e-01  1.85619316e+00\n",
      "   2.41221109e+00  2.72415407e+00  2.57097492e+00 -4.41436787e-01\n",
      "  -3.57833832e-01 -4.19444921e-03  2.69927696e-01  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-6.96456350e-01  9.45009133e-01 -3.70047164e-01  1.43692757e-01\n",
      "   1.31446701e-01  2.52849183e-02  1.94138359e-01 -1.76434874e-01\n",
      "  -1.14866706e-01 -4.80027363e-02 -1.99264087e-01  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 5.37433380e-01 -7.48167756e-01  1.85670895e+00 -1.82252840e-01\n",
      "  -5.28198068e-01 -5.83327554e-01 -5.85719550e-01  2.36670154e+00\n",
      "   1.00359874e+00 -2.76459739e-02 -1.09792279e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.16686701e+00 -6.96717630e-01 -2.10993156e-01 -2.72039863e-01\n",
      "  -5.86616793e-01 -4.33639362e-01 -3.24879703e-01  1.11523946e+00\n",
      "   8.90684297e-04 -5.18312116e-02 -1.07350408e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 6.57325500e-01 -7.71554177e-01  1.45907393e+00 -3.54344633e-01\n",
      "  -1.67949264e-01  4.44770393e-01 -1.57196944e-01 -1.07690591e+00\n",
      "  -4.52724323e-01  1.10319356e-01  5.45224568e-01  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 6.47334490e-01 -7.57522325e-01  2.75878560e-02 -9.85659635e-01\n",
      "  -7.42400059e-01  1.13853483e+00 -7.45417415e-01 -1.77211596e+00\n",
      "  -1.17438743e+00  8.02787858e-01  3.53453121e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "train_prepared.shape: (16512, 16)\n",
      "preprocess done...\n"
     ]
    }
   ],
   "source": [
    "# hands on machine learning Chap 2. practice\n",
    "# Robin Lee. 2020. 5. 20.\n",
    "\n",
    "\n",
    "# load housing data\n",
    "import os\n",
    "import pandas as pd\n",
    "HOUSING_PATH = os.path.join('datasets', 'housing')\n",
    "def load_housing_data(housing_path = HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, 'housing.csv')\n",
    "    return pd.read_csv(csv_path)\n",
    "housing = load_housing_data()\n",
    "housing.info()\n",
    "\n",
    "# split train and test set\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"]<5, 5.0, inplace=True)\n",
    "\n",
    "splits = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in splits.split(housing, housing[\"income_cat\"]):\n",
    "    train_set = housing.loc[train_index]\n",
    "    test_set = housing.loc[test_index]\n",
    "\n",
    "for set_ in (train_set, test_set, housing):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "housing_train = train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_train_label = train_set[\"median_house_value\"].copy()\n",
    "housing_train.info()\n",
    "housing_test = test_set.drop(\"median_house_value\", axis=1)\n",
    "housing_test_label = test_set[\"median_house_value\"].copy()\n",
    "\n",
    "# preprocess data - number\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "imputer = SimpleImputer(strategy = \"median\")\n",
    "housing_train_num = housing_train.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_train_num)\n",
    "\n",
    "# preprocess data - category\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "housing_train_cat = housing_train[[\"ocean_proximity\"]]\n",
    "\n",
    "# custom transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "ROOM_IDX, BEDROOM_IDX, POPULATION_IDX, HOUSEHOLDS_IDX = 3,4,5,6\n",
    "class CombinedAttributeAdder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, ROOM_IDX] / X[:, HOUSEHOLDS_IDX]\n",
    "        population_per_household = X[:, POPULATION_IDX] / X[:, HOUSEHOLDS_IDX]\n",
    "        bedrooms_per_room = X[:, BEDROOM_IDX] / X[:, ROOM_IDX]\n",
    "        return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "\n",
    "# Pandas dataframe selector\n",
    "class DataframeSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attributes):\n",
    "        self.attribute_names = attributes\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.attribute_names].values\n",
    "num_attribs = list(housing_train_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = \"median\")),\n",
    "    ('attrib_adder', CombinedAttributeAdder()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataframeSelector(cat_attribs)),\n",
    "    ('encoder', OneHotEncoder(sparse=False))\n",
    "])\n",
    "\n",
    "'''\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    ('num_pipeline', num_pipeline),\n",
    "    ('cat_pipeline', cat_pipeline)\n",
    "])\n",
    "'''\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(categories='auto'), cat_attribs)\n",
    "])\n",
    "\n",
    "train_prepared = full_pipeline.fit_transform(housing_train)\n",
    "print(train_prepared[:10])\n",
    "print(\"train_prepared.shape:\",train_prepared.shape)\n",
    "print(\"preprocess done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr rmse in train set: 69578.23550110968\n"
     ]
    }
   ],
   "source": [
    "### 1. use support vector machine\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# svr = SVR(kernel=\"linear\", C = 1000.0)      # 70256.10469475007\n",
    "svr = SVR(kernel='rbf', C=1000.0, gamma='scale')  # 69578.23550110968\n",
    "svr.fit(train_prepared, housing_train_label)\n",
    "svr_predictions = svr.predict(train_prepared)\n",
    "svr_mse = mean_squared_error(svr_predictions, housing_train_label)\n",
    "svr_rmse = np.sqrt(svr_mse)\n",
    "print(\"svr rmse in train set:\",svr_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 30000, 'kernel': 'linear'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['my_grid_svr.pkl']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 2. GridSearchCV to RadomizedSearchCV\n",
    "# Grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_params = [\n",
    "    {'kernel':['linear'], 'C':[10000, 20000,30000]},\n",
    "    #{'kernel':['rbf'], 'C':[1000, 10000, 30000], 'gamma':['scale', 'auto']}\n",
    "]\n",
    "\n",
    "svr_model = SVR()\n",
    "grid_search_svr = GridSearchCV(svr_model, grid_params, cv=5, scoring=\"neg_mean_squared_error\", return_train_score=True)\n",
    "grid_search_svr.fit(train_prepared, housing_train_label)\n",
    "print(grid_search_svr.best_params_) # {'C': 10000, 'gamma': 'scale', 'kernel': 'rbf'}\n",
    "\n",
    "import joblib\n",
    "joblib.dump(grid_search_svr, \"my_grid_svr.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 9873.86118942861}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomized Search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "random_params = {'C':uniform(loc=0, scale=40000)}\n",
    "random_search_svr = RandomizedSearchCV(svr_model, random_params, scoring=\"neg_mean_squared_error\")\n",
    "random_search_svr.fit(train_prepared, housing_train_label)\n",
    "random_search_svr.best_params_ # {'C': 9873.86118942861, 'gamma': 'scale'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median_income         0.688075\n",
      "latitude              0.144160\n",
      "total_rooms           0.134153\n",
      "housing_median_age    0.105623\n",
      "households            0.065843\n",
      "total_bedrooms        0.049686\n",
      "longitude             0.045967\n",
      "population            0.024650\n",
      "Name: median_house_value, dtype: float64\n",
      "Index(['latitude', 'housing_median_age', 'total_rooms', 'households',\n",
      "       'median_income'],\n",
      "      dtype='object')\n",
      "[1 2 3 6 7]\n",
      "[[ 0.77194962  0.74333089 -0.49323393 -0.42069842 -0.61493744]\n",
      " [ 0.6596948  -1.1653172  -0.90896655 -1.02222705  1.33645936]\n",
      " [-1.34218285  0.18664186 -0.31365989 -0.0933178  -0.5320456 ]\n",
      " [ 0.31357576 -0.29052016 -0.36276217 -0.38343559 -1.04556555]\n",
      " [-0.65929936 -0.92673619  1.85619316  2.57097492 -0.44143679]\n",
      " [ 0.94500913 -0.37004716  0.14369276  0.19413836 -0.17643487]\n",
      " [-0.74816776  1.85670895 -0.18225284 -0.58571955  2.36670154]\n",
      " [-0.69671763 -0.21099316 -0.27203986 -0.3248797   1.11523946]\n",
      " [-0.77155418  1.45907393 -0.35434463 -0.15719694 -1.07690591]\n",
      " [-0.75752232  0.02758786 -0.98565964 -0.74541742 -1.77211596]]\n",
      "[[ 0.77194962  0.74333089 -0.49323393 -0.42069842 -0.61493744]\n",
      " [ 0.6596948  -1.1653172  -0.90896655 -1.02222705  1.33645936]\n",
      " [-1.34218285  0.18664186 -0.31365989 -0.0933178  -0.5320456 ]\n",
      " [ 0.31357576 -0.29052016 -0.36276217 -0.38343559 -1.04556555]\n",
      " [-0.65929936 -0.92673619  1.85619316  2.57097492 -0.44143679]\n",
      " [ 0.94500913 -0.37004716  0.14369276  0.19413836 -0.17643487]\n",
      " [-0.74816776  1.85670895 -0.18225284 -0.58571955  2.36670154]\n",
      " [-0.69671763 -0.21099316 -0.27203986 -0.3248797   1.11523946]\n",
      " [-0.77155418  1.45907393 -0.35434463 -0.15719694 -1.07690591]\n",
      " [-0.75752232  0.02758786 -0.98565964 -0.74541742 -1.77211596]]\n"
     ]
    }
   ],
   "source": [
    "### 3. most important attribute selector\n",
    "\n",
    "# use corr as importance \n",
    "def indices_of_top_k(arr, k):\n",
    "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
    "class MostImportantAttributeSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, corr_matrix, k):\n",
    "        self.corr_matrix = corr_matrix\n",
    "        self.k = k\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # sort column by corr\n",
    "        corrr = abs(self.corr_matrix[\"median_house_value\"])\n",
    "        corrr.drop('median_house_value', inplace = True)\n",
    "        \n",
    "        # select top k attribs\n",
    "        attribs = indices_of_top_k(corrr, self.k)\n",
    "        print(attribs)\n",
    "        return X[:,attribs]\n",
    "\n",
    "corr_mat = housing.corr()\n",
    "cm = abs(corr_mat['median_house_value'])\n",
    "cm.drop('median_house_value', inplace=True)\n",
    "print(cm.sort_values(ascending=False))\n",
    "#print(housing_train.columns[indices_of_top_k(cm, 5)])\n",
    "selector = MostImportantAttributeSelector(corr_mat, 5)\n",
    "\n",
    "full_pipe = Pipeline([\n",
    "    ('full_pipeline', full_pipeline),\n",
    "    ('attrib_selector', MostImportantAttributeSelector(corr_mat, 5))\n",
    "])\n",
    "\n",
    "housing_train_prep = full_pipe.fit_transform(housing_train)\n",
    "print(housing_train_prep[:10])\n",
    "print(train_prepared[:10,indices_of_top_k(cm, 5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-4a6a152f7841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhousing_train_prep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhousing_train_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mfeature_importances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_search' is not defined"
     ]
    }
   ],
   "source": [
    "### 3. most important attribute selector (SOLUTION: assume feature importances are given...)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=8),\n",
    "    }\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "random_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs, scoring=\"neg_mean_squared_error\", n_iter=5)\n",
    "random_search.fit(housing_train_prep, housing_train_label)\n",
    "\n",
    "feature_importances = random_search.best_estimator_.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15604281,  0.77194962,  0.74333089, -0.49323393, -0.44543821],\n",
       "       [-1.17602483,  0.6596948 , -1.1653172 , -0.90896655, -1.0369278 ],\n",
       "       [ 1.18684903, -1.34218285,  0.18664186, -0.31365989, -0.15334458],\n",
       "       [-0.01706767,  0.31357576, -0.29052016, -0.36276217, -0.39675594],\n",
       "       [ 0.49247384, -0.65929936, -0.92673619,  1.85619316,  2.41221109],\n",
       "       [-0.69645635,  0.94500913, -0.37004716,  0.14369276,  0.1314467 ],\n",
       "       [ 0.53743338, -0.74816776,  1.85670895, -0.18225284, -0.52819807],\n",
       "       [ 1.16686701, -0.69671763, -0.21099316, -0.27203986, -0.58661679],\n",
       "       [ 0.6573255 , -0.77155418,  1.45907393, -0.35434463, -0.16794926],\n",
       "       [ 0.64733449, -0.75752232,  0.02758786, -0.98565964, -0.74240006]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def indices_of_top_k(arr, k):\n",
    "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
    "\n",
    "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_important, k):\n",
    "        self.feature_importances = feature_important\n",
    "        self.k = k\n",
    "    def fit(self, X, y=None):\n",
    "        self.indices = indices_of_top_k(self.feature_importances, self.k)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X[:,self.indices]\n",
    "\n",
    "preparation_and_feature_selection_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, 5))\n",
    "])\n",
    "\n",
    "housing_train_feature = preparation_and_feature_selection_pipeline.fit_transform(housing_train)\n",
    "housing_train_feature[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[418145.34175248 245692.55318833 206252.25750686 ... 278543.34923859\n",
      " 181863.70281433 164635.42000976]\n"
     ]
    }
   ],
   "source": [
    "### 4. data prepare to final predict pipeline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_train_prep, housing_train_label)\n",
    "housing_test_prep = full_pipeline.transform(housing_test)\n",
    "final_predict = lin_reg.predict(housing_train_prep)\n",
    "\n",
    "prepare_select_and_predict_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('svm_reg', SVR(C= 30000, kernel='linear'))\n",
    "])\n",
    "\n",
    "prepare_select_and_predict_pipeline.fit(housing_train, housing_train_label)\n",
    "res = prepare_select_and_predict_pipeline.predict(housing_test)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  15 out of  15 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'preparation__num__imputer__strategy': 'most_frequent'}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 5. use GridSearchCV to find option in prepare stage\n",
    "\n",
    "param_grid = [{\n",
    "    'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent']\n",
    "}]\n",
    "\n",
    "grid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n",
    "                                scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\n",
    "grid_search_prep.fit(housing_train, housing_train_label)\n",
    "\n",
    "grid_search_prep.best_params_  # {'preparation__num__imputer__strategy': 'most_frequent'}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
